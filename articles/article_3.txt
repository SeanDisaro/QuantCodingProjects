Microsoft (MSFT) has rolled out its second-generation AI chip, Maia 200, marking another step toward reducing its reliance on Nvidia (NVDA). While the move drew attention, it does not signal a move away from Nvidia anytime soon. Instead, it highlights how Big Tech is focused on gaining more control, managing costs, and planning for long-term AI growth.Maia 200 will be used inside Microsoft’s own data centers to run Copilot and AI models offered through Azure. Built by Taiwan Semiconductor Manufacturing Co. (TSM), the chip is designed for AI inference workloads, with a focus on better power efficiency and lower operating costs.In the near term, Nvidia’s position in AI remains strong. Demand for its chips continues to exceed supply, and its hardware is still the backbone for training large and complex AI models.
		At the same time, Maia 200 shows that Microsoft is widening its toolkit. Custom chips give the company more flexibility to manage costs and reduce supply risks. Over time, this could ease Nvidia’s pricing power with its largest customers.Microsoft is not alone in building its own AI chips. Alphabet (GOOGL) has spent years developing its Tensor Processing Units, while Amazon (AMZN) uses custom Trainium and Inferentia chips within AWS. In each case, the goal is to improve efficiency and lower costs within their own cloud platforms.These in-house chips are designed for specific jobs, not for every AI task. By running some workloads on their own hardware, cloud providers can lower costs and protect margins as AI use grows and power expenses increase.We used the TipRanks Stock Comparison tool to see how Wall Street analysts rate the four stocks above and which one they see as offering the strongest upside.Disclaimer & DisclosureReport an IssueCopyright © 2025
    Insider Inc and finanzen.net GmbH (Imprint). All rights reserved.
    Registration on or use of this site constitutes acceptance of our Terms of Service and Privacy Policy.